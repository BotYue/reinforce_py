Experiment 1	Target Network

DDPG use a target network to comput Target Q thus TD-error thus update critic network.

TD-error = Target_Q - Q(s, a)
	 = (reward + discount_factor * Q(next_s,next_a'|theta')) - Q(s,a|theta)

where  next_a' is the action output calculated by target actor network and Q(s',a'|theta') is calculated by target critic network.

In this exp we remove target network to verify the effect of target network. Then we got

TD-error = Target_Q - Q(s,a|theta)
	 = (reward + discounted_factor * Q(next_s,next_a|theta)) - Q(s,a|theta)

where Targer_Q was calculated using source actor and critic network.

result:
The result shows that the agent learn much more slowwly without target network. After 10000 episodes the agent got an average reawrd of about 200. By contarst, the agent can score over 9000 afer about 5000 episodes training using target networks.

___

Experiment 2	Depth of network

In this exp we verify the effect of the depth of neural networks.
Standard implementation use a neuural networks with 2 hidden layers both in actor network and critic network. The first hidden layer has 400 hidden units and the second one has 300.

First we reduce to 1 hidden layer with 400 units for both networks.
After 10000 episode tarining, the average reward is about 30. It seems the network can't learn anything.

Then we remained the critic network unchanged adn reduce the depth of actor network.
The result is superisingly good. The agent solved the problem (score over 9100 in 100 consecutice episodes) within 4500 episodes. This result suggest that the the depth of network has a greater impact on critic netwrok.

Also we reduce the depth of critic network and remain actor unchanged. After 10000 ep training, the agant basically did not learning anything. The score is closed to random policy. All these three exp shows that critic network need more hidden unit to approximate Q value, thus speed up learning progress.

___

Experiments 3 Reguralization

We add regularization to the critic network, it turns out that this even hurts the learning. (the agent can hardly make some progress) We suspect that in DRL context with a large state space, the network is almost impossiable to overfitting. Thus regularization did not benifit learning.

TODO:
regularizate actor network?

___

Experiment 4	Learning rate of the optimizer

We use a AdamOptimizer to train both actor network and critic network. In this experimetn we test the effects of different learning rates.

actor_1e-4, critic_1e-3 > both_1e-4

___

Experiment 5 Batch Normalization

After some control experiments, we found that using batch normalization only in actor network works better.

Note that when apply batch normalization, we should switch the 'is_training' flag while sampling action and training model.

Also note that the batch_norm parameters of target networks should not be update while apply soft updating.
  
